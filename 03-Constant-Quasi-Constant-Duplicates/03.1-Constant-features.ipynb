{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant features\n",
    "\n",
    "Constant features are those that show the same value, just one value, for all the observations of the dataset. In other words, the same value for all the rows of the dataset. These features provide no information that allows a machine learning model to discriminate or predict a target.\n",
    "\n",
    "Identifying and removing constant features is an easy first step towards feature selection and more easily interpretable machine learning models.\n",
    "\n",
    "Here, I will demonstrate how to identify constant features using a dataset that I created for this course. \n",
    "\n",
    "To identify constant features, we can use the VarianceThreshold from Scikit-learn, or we can code it ourselves. If using the VarianceThreshold, all our variables need to be numerical. If we do it manually however, we can apply the code to both numerical and categorical variables.\n",
    "\n",
    "I will show 3 snippets of code, 1 where I use the VarianceThreshold and 2 manually coded alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our first dataset\n",
    "\n",
    "# (feel free to write some code to explore the dataset and become\n",
    "# familiar with it ahead of this demo)\n",
    "\n",
    "data = pd.read_csv('../dataset_1.csv')\n",
    "data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate dataset into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),  # drop the target\n",
    "    data['target'],  # just the target\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VarianceThreshold from Scikit-learn\n",
    "\n",
    "The VarianceThreshold from sklearn provides a simple baseline approach to feature selection. It removes all features which variance doesnâ€™t meet a certain threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = VarianceThreshold(threshold=0)\n",
    "\n",
    "sel.fit(X_train)  # fit finds the features with zero variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_support is a boolean vector that indicates which features are retained\n",
    "# if we sum over get_support, we get the number of features that are not constant\n",
    "\n",
    "# (go ahead and print the result of sel.get_support() to understand its output)\n",
    "\n",
    "sum(sel.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's print the number of constant feautures\n",
    "# (see how we use ~ to exclude non-constant features)\n",
    "\n",
    "constant = X_train.columns[~sel.get_support()]\n",
    "\n",
    "len(constant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 34 columns / variables are constant. This means that 34 variables show the same value, just one value, for all the observations of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's print the constant variable names\n",
    "\n",
    "constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualise the values of one of the constant variables\n",
    "# as an example\n",
    "\n",
    "X_train['var_23'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we can do the same for every feature:\n",
    "\n",
    "for col in constant:\n",
    "    print(col, X_train[col].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the transform() method of the VarianceThreshold to reduce the training and testing sets to its non-constant features.\n",
    "\n",
    "Note that VarianceThreshold returns a NumPy array without feature names, so we need to capture the names first, and reconstitute the dataframe in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture non-constant feature names\n",
    "\n",
    "feat_names = X_train.columns[sel.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We passed from our original 300 variables, to 266."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_ train is a NumPy array\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstitute de dataframe\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=feat_names)\n",
    "X_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual code 1: only works with numerical\n",
    "\n",
    "In the following cells, I will show an alternative to the VarianceThreshold transformer of sklearn, were we write the code to find out constant variables, using the standard deviation from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate train and test (again, as we transformed the previous ones)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# short and easy: find constant features\n",
    "\n",
    "# in this dataset, all features are numeric,\n",
    "# so this bit of code will suffice:\n",
    "\n",
    "constant_features = [\n",
    "    feat for feat in X_train.columns if X_train[feat].std() == 0\n",
    "]\n",
    "\n",
    "len(constant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop these columns from the train and test sets:\n",
    "\n",
    "X_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=constant_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how by removing constant features, we managed to reduced the feature space quite a bit.\n",
    "\n",
    "Both the VarianceThreshold and the snippet of code I provided work with numerical variables. What can we do to find constant categorical variables?\n",
    "\n",
    "One alternative is to encode the categories as numbers and then use the code above. But then you will put effort in pre-processing variables that are not informative.\n",
    "\n",
    "The code below offers a better solution:\n",
    "\n",
    "### Manual Code 2 - works also with categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate train and test (again, as we transformed the previous ones)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I will cast all the numeric features as object,\n",
    "# to simulate that they are categorical\n",
    "\n",
    "X_train = X_train.astype('O')\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to find variables that contain only 1 label/value\n",
    "# we use the nunique() method from pandas, which returns the number\n",
    "# of different values in a variable.\n",
    "\n",
    "constant_features = [\n",
    "    feat for feat in X_train.columns if X_train[feat].nunique() == 1\n",
    "]\n",
    "\n",
    "len(constant_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, we observe 34 variables that show only 1 value in all the observations of the dataset. Like this, we can appreciate the usefulness of looking out for constant variables at the beginning of any modeling exercise.\n",
    "\n",
    "**Note** by default nunique() ignores missing values, so if your variables have missing values, use dropna=False within the parameters of nunique(). More details here:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.nunique.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=constant_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all for this lecture, I hope you enjoyed it and see you in the next one!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "583px",
    "left": "0px",
    "right": "20px",
    "top": "107px",
    "width": "319px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
