{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "Correlation Feature Selection evaluates subsets of features on the basis of the following hypothesis: \"Good feature subsets contain features highly correlated with the target, yet uncorrelated to each other\".\n",
    "\n",
    "**References**:\n",
    "\n",
    "M. Hall 1999, [Correlation-based Feature Selection for Machine Learning](http://www.cs.waikato.ac.nz/~mhall/thesis.pdf)\n",
    "\n",
    "Senliol, Baris, et al. \"Fast Correlation Based Filter (FCBF) with a different search strategy.\" Computer and Information Sciences.\n",
    "\n",
    "\n",
    "I will demonstrate how to select features based on correlation using 2 procedures:\n",
    "\n",
    "The first one is a brute force function that finds correlated features without any further insight. \n",
    "\n",
    "The second procedure finds groups of correlated features, which we can then explore to decide which one we keep and which ones we discard.\n",
    "\n",
    "Often, more than 2 features are correlated with each other. We can find groups of 3, 4 or more features that are correlated. By identifying these groups, with procedure 2, we can then select from each group, which feature we want to keep, and which ones we want to remove.\n",
    "\n",
    "**Note**\n",
    "\n",
    "The most used method to determine correlation is the Pearson's correlation method, which is the one that I will carry out in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "data = pd.read_csv('../dataset_2.csv', nrows=50000)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, feature selection should be done after data pre-processing, so ideally, all the categorical variables are encoded into numbers, and then you can asses whether they are correlated with other features.\n",
    "\n",
    "This dataset, for simplicity, contains only numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise correlated features:\n",
    "\n",
    "# I will build a correlation matrix, which examines the \n",
    "# correlation of all features (that is, for all possible feature combinations)\n",
    "# and then visualise the correlation matrix using a heatmap\n",
    "\n",
    "# the default correlation method of pandas.corr is pearson\n",
    "# I include it anyways for the demo\n",
    "corrmat = X_train.corr(method='pearson')\n",
    "\n",
    "# we can make a heatmap with the package seaborn\n",
    "# and customise the colours of searborn's heatmap\n",
    "cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "\n",
    "# some more parameters for the figure\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(11,11)\n",
    "\n",
    "# and now plot the correlation matrix\n",
    "sns.heatmap(corrmat, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, the dark orange squares correspond to highly correlated features (>0.8). Darker blue squares corresponds to negatively correlated features (<-0.8).\n",
    "\n",
    "The diagonal represents the correlation of a feature with itself, therefore the value is 1.\n",
    "\n",
    "We can see that there are a few features that are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the heatmap, we see that var25 is highly correlated \n",
    "# with others in the dataset\n",
    "\n",
    "# with this code we can get the other variable names\n",
    "# and also print the correlation coefficient between var25 and \n",
    "# these variables:\n",
    "\n",
    "c = 0\n",
    "\n",
    "# iterate over each correlation value for var_5:\n",
    "for i in corrmat.loc['var_5']:\n",
    "    \n",
    "    # if highly correlated\n",
    "    if i>0.8:\n",
    "        \n",
    "        # print the variable name and the correlation coefficient\n",
    "        print(corrmat.columns[c], i)\n",
    "        \n",
    "    c = c +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's plot the variables\n",
    "\n",
    "plt.scatter(X_train['var_5'], X_train['var_28'])\n",
    "plt.ylabel('var_28')\n",
    "plt.xlabel('var_5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's plot the variables\n",
    "\n",
    "plt.scatter(X_train['var_5'], X_train['var_75'])\n",
    "plt.ylabel('var_75')\n",
    "plt.xlabel('var_5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the correlation of the second pair of variables is not quite linear.\n",
    "\n",
    "Next, let's identify a couple of negatively correlated variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in corrmat.loc['var_88']:\n",
    "    \n",
    "    # if highly and negatively correlated\n",
    "    if i<-0.8:\n",
    "        \n",
    "        # print the variable name and the correlation coefficient\n",
    "        print(corrmat.columns[c], i)\n",
    "        \n",
    "    c = c +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's plot the variables\n",
    "\n",
    "plt.scatter(X_train['var_88'], X_train['var_70'])\n",
    "plt.ylabel('var_70')\n",
    "plt.xlabel('var_88')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train['var_88'], X_train['var_21'])\n",
    "plt.ylabel('var_21')\n",
    "plt.xlabel('var_88')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the value of 1 variable, the lower the value of the other variable.\n",
    "\n",
    "Now let's proceed with feature selection\n",
    "\n",
    "## Remove correlated\n",
    "\n",
    "### Brute force approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the following function we can select highly correlated features\n",
    "# it will remove the first feature that is correlated with anything else\n",
    "# without any further insight.\n",
    "\n",
    "def correlation(dataset, threshold):\n",
    "    \n",
    "    # create a set where I will store the names of correlated columns\n",
    "    col_corr = set()\n",
    "    \n",
    "    # create the correlation matrix\n",
    "    corr_matrix = dataset.corr()\n",
    "    \n",
    "    # for each feature in the dataset (columns of the correlation matrix)\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        \n",
    "        # check with other features\n",
    "        for j in range(i):\n",
    "            \n",
    "            # if the correlation is higher than a certain threshold\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                \n",
    "                # print correlation, and variables examined\n",
    "                # keep in mind that the columns and rows of the dataframe are identical\n",
    "                # so we can identify the features being examned by looking for i,j\n",
    "                # in the column names\n",
    "                print(abs(corr_matrix.iloc[i, j]), corr_matrix.columns[i], corr_matrix.columns[j])\n",
    "                \n",
    "                # get the name of the correlated feature\n",
    "                colname = corr_matrix.columns[j]\n",
    "                \n",
    "                # and add it to our correlated set\n",
    "                col_corr.add(colname)\n",
    "                \n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**, if the code in cell 50 is hard to understand, either run the code line by line (you would have to manually enter some of the values of i), or add print statements within the loop to print the intermediate outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_features = correlation(X_train, 0.8)\n",
    "len(set(corr_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 40 features are highly correlated with other features in the training set. \n",
    "\n",
    "Very likely, by removing these correlated features, the performance of your machine learning models will drop very little, if at all. We can go ahead and drop the features like we have done in previous lectures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing correlated columns we reduced the feature space from 108 numerical columns to 68.\n",
    "\n",
    "### Second approach\n",
    "\n",
    "The second approach looks to identify groups of highly correlated features. And then, we can make further investigation within these groups to decide which feature we keep and which one we remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dataframe with the correlation between features\n",
    "# remember that the absolute value of the correlation\n",
    "# coefficient is important and not the sign\n",
    "\n",
    "corrmat = X_train.corr()\n",
    "corrmat = corrmat.abs().unstack() # absolute value of corr coef\n",
    "corrmat = corrmat.sort_values(ascending=False)\n",
    "corrmat = corrmat[corrmat >= 0.8]\n",
    "corrmat = corrmat[corrmat < 1]\n",
    "corrmat = pd.DataFrame(corrmat).reset_index()\n",
    "corrmat.columns = ['feature1', 'feature2', 'corr']\n",
    "corrmat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**, if the code above is not clear, run each command individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find groups of correlated features\n",
    "\n",
    "grouped_feature_ls = []\n",
    "correlated_groups = []\n",
    "\n",
    "for feature in corrmat.feature1.unique():\n",
    "    \n",
    "    if feature not in grouped_feature_ls:\n",
    "\n",
    "        # find all features correlated to a single feature\n",
    "        correlated_block = corrmat[corrmat.feature1 == feature]\n",
    "        grouped_feature_ls = grouped_feature_ls + list(\n",
    "            correlated_block.feature2.unique()) + [feature]\n",
    "\n",
    "        # append the block of features to the list\n",
    "        correlated_groups.append(correlated_block)\n",
    "\n",
    "print('found {} correlated groups'.format(len(correlated_groups)))\n",
    "print('out of {} total features'.format(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now we can print out each group. We see that some groups contain\n",
    "# only 2 correlated features, some other groups present several features \n",
    "# that are correlated among themselves.\n",
    "\n",
    "for group in correlated_groups:\n",
    "    print(group)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now investigate further features within one group.\n",
    "# let's for example select group 1\n",
    "\n",
    "group = correlated_groups[1]\n",
    "group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this group, several features are highly correlated. Which one should we keep and which ones should we remove?**\n",
    "\n",
    "One criteria to select which features to use from this group, would be to use those with **less missing data**. \n",
    "\n",
    "Our dataset contains no missing values, so this is not an option. But keep this in mind when you work with your own datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could build a **machine learning algorithm using all the features from the above list, and select the more predictive one**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# add all features of the group to a list\n",
    "features = list(group['feature2'].unique())+['var_32']\n",
    "\n",
    "# train a random forest \n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
    "rf.fit(X_train[features].fillna(0), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature importance attributed by the \n",
    "# random forest model (more on this in coming lectures)\n",
    "\n",
    "importance = pd.concat(\n",
    "    [pd.Series(features),\n",
    "     pd.Series(rf.feature_importances_)], axis=1)\n",
    "\n",
    "importance.columns = ['feature', 'importance']\n",
    "\n",
    "# sort features by importance, most important first\n",
    "importance.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, feature var_39 shows the highest importance according to random forests. Then, I would select var_39, and remove all the remaining features from this group from the dataset.\n",
    "\n",
    "**Note**\n",
    "\n",
    "None of the 2 procedures for removing correlated features are perfect, and some correlated features may escape the loops of code. So it might be worthwhile checking that after removing the correlated features, there are no correlated features left in the dataset. If there are, repeat the procedure to remove the remaining ones.\n",
    "\n",
    "That is all for this lecture, I hope you enjoyed it and see you in the next one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS:\n",
    "# let's plot the correlation of the features in the correlated group from the\n",
    "# previous cell\n",
    "\n",
    "for feature in group['feature2']:\n",
    "    plt.scatter(X_train['var_32'], X_train[feature])\n",
    "    plt.xlabel('var_22')\n",
    "    plt.ylabel(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241.6px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
